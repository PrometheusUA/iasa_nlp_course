{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import gensim\n",
    "import optuna\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from lightgbm import LGBMRegressor\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from scipy import spatial\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARIES_TRAIN_FILE = '../../data/commonlit_evaluate_student_summaries/summaries_train.csv'\n",
    "SUMMARIES_TEST_FILE = '../../data/commonlit_evaluate_student_summaries/summaries_test.csv'\n",
    "PROMPTS_TRAIN_FILE = '../../data/commonlit_evaluate_student_summaries/prompts_train.csv'\n",
    "PROMPTS_TEST_FILE = '../../data/commonlit_evaluate_student_summaries/prompts_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df = pd.read_csv(SUMMARIES_TRAIN_FILE)\n",
    "summaries_test_df = pd.read_csv(SUMMARIES_TEST_FILE)\n",
    "prompts_train_df = pd.read_csv(PROMPTS_TRAIN_FILE)\n",
    "prompts_test_df = pd.read_csv(PROMPTS_TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'good', 'bad', 'people']) #stopwords extended a bit\n",
    "def preprocess_hard_base(text, join_back=True):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        # Stop Words Cleaning\n",
    "        if (\n",
    "            token not in gensim.parsing.preprocessing.STOPWORDS and \n",
    "            token not in stop_words\n",
    "        ):\n",
    "            result.append(token)\n",
    "    if join_back:\n",
    "        result = \" \".join(result)\n",
    "    return result\n",
    "\n",
    "def preprocess_hard_stemming(text, join_back=True, stemmer = PorterStemmer()):\n",
    "    tokens = preprocess_hard_base(text, join_back=False)\n",
    "    \n",
    "    result = [stemmer.stem(word) for word in tokens]\n",
    "    if join_back:\n",
    "        result = \" \".join(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_dots(text):\n",
    "    # Collapse sequential dots\n",
    "    input = re.sub(\"\\.+\", \".\", text)\n",
    "    # Collapse dots separated by whitespaces\n",
    "    all_collapsed = False\n",
    "    while not all_collapsed:\n",
    "        output = re.sub(r\"\\.(( )*)\\.\", \".\", text)\n",
    "        all_collapsed = input == output\n",
    "        input = output\n",
    "    return output\n",
    "\n",
    "# Check how it will influence different ML models\n",
    "def process_soft(text):\n",
    "    if isinstance(text, str):\n",
    "        text = \" \".join(tokenize.sent_tokenize(text))\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = re.sub(r\"\\n+\", \". \", text)\n",
    "        for symb in [\"!\", \",\", \":\", \";\", \"?\"]:\n",
    "            text = re.sub(rf\"\\{symb}\\.\", symb, text)\n",
    "        text = re.sub(\"[^а-яА-Яa-zA-Z0-9!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~ё]+\", \" \", text)\n",
    "        text = re.sub(r\"#\\S+\", \"\", text)\n",
    "        text = collapse_dots(text)\n",
    "        text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df['text_soft_preprocessed'] = summaries_train_df['text'].apply(process_soft)\n",
    "summaries_test_df['text_soft_preprocessed'] = summaries_test_df['text'].apply(process_soft)\n",
    "summaries_train_df['text_hard_preprocessed_stemmed'] = summaries_train_df['text'].apply(preprocess_hard_stemming)\n",
    "summaries_test_df['text_hard_preprocessed_stemmed'] = summaries_test_df['text'].apply(preprocess_hard_stemming)\n",
    "\n",
    "summaries_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_train_df['prompt_question_soft_preprocessed'] = prompts_train_df['prompt_question'].apply(process_soft)\n",
    "prompts_train_df['prompt_title_soft_preprocessed'] = prompts_train_df['prompt_title'].apply(process_soft)\n",
    "prompts_train_df['prompt_text_soft_preprocessed'] = prompts_train_df['prompt_text'].apply(process_soft)\n",
    "\n",
    "prompts_test_df['prompt_question_soft_preprocessed'] = prompts_test_df['prompt_question'].apply(process_soft)\n",
    "prompts_test_df['prompt_title_soft_preprocessed'] = prompts_test_df['prompt_title'].apply(process_soft)\n",
    "prompts_test_df['prompt_text_soft_preprocessed'] = prompts_test_df['prompt_text'].apply(process_soft)\n",
    "\n",
    "prompts_train_df['prompt_question_hard_preprocessed_stemmed'] = prompts_train_df['prompt_question'].apply(preprocess_hard_stemming)\n",
    "prompts_train_df['prompt_title_hard_preprocessed_stemmed'] = prompts_train_df['prompt_title'].apply(preprocess_hard_stemming)\n",
    "prompts_train_df['prompt_text_hard_preprocessed_stemmed'] = prompts_train_df['prompt_text'].apply(preprocess_hard_stemming)\n",
    "\n",
    "prompts_test_df['prompt_question_hard_preprocessed_stemmed'] = prompts_test_df['prompt_question'].apply(preprocess_hard_stemming)\n",
    "prompts_test_df['prompt_title_hard_preprocessed_stemmed'] = prompts_test_df['prompt_title'].apply(preprocess_hard_stemming)\n",
    "prompts_test_df['prompt_text_hard_preprocessed_stemmed'] = prompts_test_df['prompt_text'].apply(preprocess_hard_stemming)\n",
    "\n",
    "prompts_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the stop words in the text.\n",
    "def count_stopwords(text: str) -> int:\n",
    "    stopword_list = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    stopwords_count = sum(1 for word in words if word.lower() in stopword_list)\n",
    "    return stopwords_count\n",
    "\n",
    "# Count the punctuations in the text.\n",
    "# punctuation_set -> !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "def count_punctuation(text: str) -> int:\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    punctuation_count = sum(1 for char in text if char in punctuation_set)\n",
    "    return punctuation_count\n",
    "\n",
    "# Count the digits in the text.\n",
    "def count_numbers(text: str) -> int:\n",
    "    numbers = re.findall(r'\\d+', text)\n",
    "    numbers_count = len(numbers)\n",
    "    return numbers_count\n",
    "\n",
    "# This function applies all the above preprocessing functions on a text feature.\n",
    "def streamlit_feature_engineer(dataframe: pd.DataFrame, feature: str = 'text', preprocessed_hard: bool = False) -> pd.DataFrame:\n",
    "    dataframe[f'{feature}_word_cnt'] = dataframe[feature].apply(lambda x: len(x.split(' ')))\n",
    "    dataframe[f'{feature}_length'] = dataframe[feature].apply(lambda x: len(x))\n",
    "    if not preprocessed_hard:\n",
    "        dataframe[f'{feature}_stopword_cnt'] = dataframe[feature].apply(lambda x: count_stopwords(x))\n",
    "        dataframe[f'{feature}_punct_cnt'] = dataframe[feature].apply(lambda x: count_punctuation(x))\n",
    "        dataframe[f'{feature}_number_cnt'] = dataframe[feature].apply(lambda x: count_numbers(x))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train_df = streamlit_feature_engineer(summaries_train_df, feature = \"text_soft_preprocessed\")\n",
    "summaries_test_df = streamlit_feature_engineer(summaries_test_df, feature = \"text_soft_preprocessed\")\n",
    "summaries_train_df = streamlit_feature_engineer(summaries_train_df, feature = \"text_hard_preprocessed_stemmed\", preprocessed_hard=True)\n",
    "summaries_test_df = streamlit_feature_engineer(summaries_test_df, feature = \"text_hard_preprocessed_stemmed\", preprocessed_hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_ids_to_is = {prompt_id: i for i, prompt_id in zip(prompts_train_df.index, prompts_train_df['prompt_id'])}\n",
    "summaries_train_df['prompt_i'] = summaries_train_df['prompt_id'].apply(lambda prompt_id: prompts_ids_to_is[prompt_id])\n",
    "prompts_ids_to_is_test = {prompt_id: i for i, prompt_id in zip(prompts_test_df.index, prompts_test_df['prompt_id'])}\n",
    "summaries_test_df['prompt_i'] = summaries_test_df['prompt_id'].apply(lambda prompt_id: prompts_ids_to_is_test[prompt_id])\n",
    "summaries_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper class to match sklearn's interface\n",
    "class SentenceTransformerVectorizer:\n",
    "    def __init__(self, model='all-MiniLM-L6-v2', device=\"cuda\"):\n",
    "        self.sent_tr = SentenceTransformer(model,device=device)\n",
    "\n",
    "    def fit(self, texts):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts):\n",
    "        encoded_texts = self.sent_tr.encode(texts.to_numpy())\n",
    "        return sparse.csr_matrix(encoded_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['text_soft_preprocessed', 'text_soft_preprocessed_word_cnt', 'text_soft_preprocessed_length', 'text_soft_preprocessed_stopword_cnt', 'text_soft_preprocessed_punct_cnt', 'text_soft_preprocessed_number_cnt']\n",
    "features_to_scale = ['text_soft_preprocessed_word_cnt', 'text_soft_preprocessed_length', 'text_soft_preprocessed_stopword_cnt', 'text_soft_preprocessed_punct_cnt', 'text_soft_preprocessed_number_cnt']\n",
    "vectorizer = SentenceTransformerVectorizer()\n",
    "vectorizer_feature = \"text_soft_preprocessed\"\n",
    "prompt_processed_features = {\n",
    "  'prompt_question': 'prompt_question_soft_preprocessed',\n",
    "  'prompt_title': 'prompt_title_soft_preprocessed',\n",
    "  'prompt_text': 'prompt_text_soft_preprocessed'\n",
    "}\n",
    "target = \"content\"\n",
    "lightgbm_content_params = {\n",
    "    'lambda_l1': 0.1296119760419726, \n",
    "    'lambda_l2': 0.049549534856476465, \n",
    "    'learning_rate': 0.03336396209486242, \n",
    "    'num_leaves': 140, \n",
    "    'max_depth': 1, \n",
    "    'n_estimators': 186, \n",
    "    'feature_fraction': 0.981443658599143, \n",
    "    'bagging_fraction': 0.6007548488211474, \n",
    "    'bagging_freq_1': 12, \n",
    "    'min_child_samples': 64\n",
    "}\n",
    "\n",
    "X_train, y_train = summaries_train_df.loc[:, ['prompt_i', *features]], summaries_train_df.loc[:, target]\n",
    "X_test = summaries_test_df.loc[:, ['prompt_i', *features]]\n",
    "\n",
    "vectorizer = vectorizer.fit(X_train[vectorizer_feature])\n",
    "train_summaries_vectors = vectorizer.transform(X_train[vectorizer_feature])\n",
    "test_summaries_vectors = vectorizer.transform(X_test[vectorizer_feature])\n",
    "\n",
    "prompts_texts_vectors = vectorizer.transform(prompts_train_df[prompt_processed_features['prompt_text']])\n",
    "prompts_titles_vectors = vectorizer.transform(prompts_train_df[prompt_processed_features['prompt_title']])\n",
    "prompts_questions_vectors = vectorizer.transform(prompts_train_df[prompt_processed_features['prompt_question']])\n",
    "\n",
    "scaler = RobustScaler().fit(X_train[features_to_scale])\n",
    "X_train[features_to_scale] = scaler.transform(X_train[features_to_scale])\n",
    "X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "y_scaler = RobustScaler().fit(y_train.to_numpy().reshape(-1, 1))\n",
    "y_train_scaled = y_scaler.transform(y_train.to_numpy().reshape(-1, 1))\n",
    "\n",
    "train_summaries_vectors_dense = train_summaries_vectors.todense()\n",
    "test_summaries_vectors_dense = test_summaries_vectors.todense()\n",
    "\n",
    "cosine_scores_train_prompts_texts = np.zeros((len(y_train), 1))\n",
    "cosine_scores_train_prompts_titles = np.zeros((len(y_train), 1))\n",
    "cosine_scores_train_prompts_questions = np.zeros((len(y_train), 1))\n",
    "\n",
    "for i, (summary_vector, prompt_text_vector, prompt_title_vector, prompt_question_vector) in enumerate(zip(train_summaries_vectors_dense, prompts_texts_vectors[X_train['prompt_i']].todense(), prompts_titles_vectors[X_train['prompt_i']].todense(), prompts_questions_vectors[X_train['prompt_i']].todense())):\n",
    "    cosine_scores_train_prompts_texts[i, 0] = cosine_similarity(np.asarray(prompt_text_vector), np.asarray(summary_vector))\n",
    "    cosine_scores_train_prompts_titles[i, 0] = cosine_similarity(np.asarray(prompt_title_vector), np.asarray(summary_vector))\n",
    "    cosine_scores_train_prompts_questions[i, 0] = cosine_similarity(np.asarray(prompt_question_vector), np.asarray(summary_vector))\n",
    "\n",
    "cosine_scores_test_prompts_texts = np.zeros((X_test.shape[0], 1))\n",
    "cosine_scores_test_prompts_titles = np.zeros((X_test.shape[0], 1))\n",
    "cosine_scores_test_prompts_questions = np.zeros((X_test.shape[0], 1))\n",
    "\n",
    "for i, (summary_vector, prompt_text_vector, prompt_title_vector, prompt_question_vector) in enumerate(zip(test_summaries_vectors_dense, prompts_texts_vectors[X_test['prompt_i']].todense(), prompts_titles_vectors[X_test['prompt_i']].todense(), prompts_questions_vectors[X_test['prompt_i']].todense())):\n",
    "    cosine_scores_test_prompts_texts[i, 0] = cosine_similarity(np.asarray(prompt_text_vector), np.asarray(summary_vector))\n",
    "    cosine_scores_test_prompts_titles[i, 0] = cosine_similarity(np.asarray(prompt_title_vector), np.asarray(summary_vector))\n",
    "    cosine_scores_test_prompts_questions[i, 0] = cosine_similarity(np.asarray(prompt_question_vector), np.asarray(summary_vector))\n",
    "\n",
    "X_train = sparse.hstack((\n",
    "    train_summaries_vectors,\n",
    "    sparse.coo_matrix(cosine_scores_train_prompts_texts),\n",
    "    sparse.coo_matrix(cosine_scores_train_prompts_titles),\n",
    "    sparse.coo_matrix(cosine_scores_train_prompts_questions),\n",
    "    sparse.coo_matrix(X_train[features_to_scale].to_numpy()),\n",
    "))\n",
    "X_test = sparse.hstack((\n",
    "    test_summaries_vectors,\n",
    "    sparse.coo_matrix(cosine_scores_test_prompts_texts),\n",
    "    sparse.coo_matrix(cosine_scores_test_prompts_titles),\n",
    "    sparse.coo_matrix(cosine_scores_test_prompts_questions),\n",
    "    sparse.coo_matrix(X_test[features_to_scale].to_numpy()),\n",
    "))\n",
    "\n",
    "model = LGBMRegressor(**lightgbm_content_params, verbose = -1)\n",
    "model.fit(X_train, y_train_scaled)\n",
    "y_train_pred_scaled = model.predict(X_train)\n",
    "y_test_pred_scaled = model.predict(X_test)\n",
    "\n",
    "y_test_pred_content = y_scaler.inverse_transform(y_test_pred_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['text_hard_preprocessed_stemmed', 'text_hard_preprocessed_stemmed_word_cnt', 'text_hard_preprocessed_stemmed_length', 'text_soft_preprocessed_stopword_cnt', 'text_soft_preprocessed_punct_cnt', 'text_soft_preprocessed_number_cnt']\n",
    "features_to_scale = ['text_hard_preprocessed_stemmed_word_cnt', 'text_hard_preprocessed_stemmed_length', 'text_soft_preprocessed_stopword_cnt', 'text_soft_preprocessed_punct_cnt', 'text_soft_preprocessed_number_cnt']\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer='word',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 3),\n",
    "    lowercase=True,\n",
    "    min_df=1,\n",
    "    max_features=30000\n",
    ")\n",
    "vectorizer_feature = \"text_hard_preprocessed_stemmed\"\n",
    "prompt_processed_features = {\n",
    "  'prompt_question': 'prompt_question_hard_preprocessed_stemmed',\n",
    "  'prompt_title': 'prompt_title_hard_preprocessed_stemmed',\n",
    "  'prompt_text': 'prompt_text_hard_preprocessed_stemmed'\n",
    "}\n",
    "target = \"wording\"\n",
    "lightgbm_wording_params={\n",
    "    'lambda_l1': 0.5499102131489506, \n",
    "    'lambda_l2': 0.13682389299339068, \n",
    "    'learning_rate': 0.179791006382902, \n",
    "    'num_leaves': 2, \n",
    "    'max_depth': 1, \n",
    "    'n_estimators': 121, \n",
    "    'feature_fraction': 0.7792526204040188, \n",
    "    'bagging_fraction': 0.41045226793565176, \n",
    "    'bagging_freq_1': 20, \n",
    "    'min_child_samples': 45\n",
    "}\n",
    "\n",
    "X_train, y_train = summaries_train_df.loc[:, ['prompt_i', *features]], summaries_train_df.loc[:, target]\n",
    "X_test = summaries_test_df.loc[:, ['prompt_i', *features]]\n",
    "\n",
    "vectorizer = vectorizer.fit(X_train[vectorizer_feature])\n",
    "train_summaries_vectors = vectorizer.transform(X_train[vectorizer_feature])\n",
    "test_summaries_vectors = vectorizer.transform(X_test[vectorizer_feature])\n",
    "\n",
    "prompts_texts_vectors = vectorizer.transform(prompts_train_df[prompt_processed_features['prompt_text']])\n",
    "prompts_titles_vectors = vectorizer.transform(prompts_train_df[prompt_processed_features['prompt_title']])\n",
    "prompts_questions_vectors = vectorizer.transform(prompts_train_df[prompt_processed_features['prompt_question']])\n",
    "\n",
    "scaler = RobustScaler().fit(X_train[features_to_scale])\n",
    "X_train[features_to_scale] = scaler.transform(X_train[features_to_scale])\n",
    "X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])\n",
    "\n",
    "y_scaler = RobustScaler().fit(y_train.to_numpy().reshape(-1, 1))\n",
    "y_train_scaled = y_scaler.transform(y_train.to_numpy().reshape(-1, 1))\n",
    "\n",
    "train_summaries_vectors_dense = train_summaries_vectors.todense()\n",
    "test_summaries_vectors_dense = test_summaries_vectors.todense()\n",
    "\n",
    "cosine_scores_train_prompts_texts = np.zeros((len(y_train), 1))\n",
    "cosine_scores_train_prompts_titles = np.zeros((len(y_train), 1))\n",
    "cosine_scores_train_prompts_questions = np.zeros((len(y_train), 1))\n",
    "\n",
    "for i, (summary_vector, prompt_text_vector, prompt_title_vector, prompt_question_vector) in enumerate(zip(train_summaries_vectors_dense, prompts_texts_vectors[X_train['prompt_i']].todense(), prompts_titles_vectors[X_train['prompt_i']].todense(), prompts_questions_vectors[X_train['prompt_i']].todense())):\n",
    "    cosine_scores_train_prompts_texts[i, 0] = cosine_similarity(np.asarray(prompt_text_vector), np.asarray(summary_vector))\n",
    "    cosine_scores_train_prompts_titles[i, 0] = cosine_similarity(np.asarray(prompt_title_vector), np.asarray(summary_vector))\n",
    "    cosine_scores_train_prompts_questions[i, 0] = cosine_similarity(np.asarray(prompt_question_vector), np.asarray(summary_vector))\n",
    "\n",
    "cosine_scores_test_prompts_texts = np.zeros((X_test.shape[0], 1))\n",
    "cosine_scores_test_prompts_titles = np.zeros((X_test.shape[0], 1))\n",
    "cosine_scores_test_prompts_questions = np.zeros((X_test.shape[0], 1))\n",
    "\n",
    "for i, (summary_vector, prompt_text_vector, prompt_title_vector, prompt_question_vector) in enumerate(zip(test_summaries_vectors_dense, prompts_texts_vectors[X_test['prompt_i']].todense(), prompts_titles_vectors[X_test['prompt_i']].todense(), prompts_questions_vectors[X_test['prompt_i']].todense())):\n",
    "    cosine_scores_test_prompts_texts[i, 0] = cosine_similarity(np.asarray(prompt_text_vector), np.asarray(summary_vector))\n",
    "    cosine_scores_test_prompts_titles[i, 0] = cosine_similarity(np.asarray(prompt_title_vector), np.asarray(summary_vector))\n",
    "    cosine_scores_test_prompts_questions[i, 0] = cosine_similarity(np.asarray(prompt_question_vector), np.asarray(summary_vector))\n",
    "\n",
    "X_train = sparse.hstack((\n",
    "    train_summaries_vectors,\n",
    "    sparse.coo_matrix(cosine_scores_train_prompts_texts),\n",
    "    sparse.coo_matrix(cosine_scores_train_prompts_titles),\n",
    "    sparse.coo_matrix(cosine_scores_train_prompts_questions),\n",
    "    sparse.coo_matrix(X_train[features_to_scale].to_numpy()),\n",
    "))\n",
    "X_test = sparse.hstack((\n",
    "    test_summaries_vectors,\n",
    "    sparse.coo_matrix(cosine_scores_test_prompts_texts),\n",
    "    sparse.coo_matrix(cosine_scores_test_prompts_titles),\n",
    "    sparse.coo_matrix(cosine_scores_test_prompts_questions),\n",
    "    sparse.coo_matrix(X_test[features_to_scale].to_numpy()),\n",
    "))\n",
    "\n",
    "model = LGBMRegressor(**lightgbm_wording_params, verbose = -1)\n",
    "model.fit(X_train, y_train_scaled)\n",
    "y_train_pred_scaled = model.predict(X_train)\n",
    "y_test_pred_scaled = model.predict(X_test)\n",
    "\n",
    "y_test_pred_wording = y_scaler.inverse_transform(y_test_pred_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_test_df['content'] = y_test_pred_content\n",
    "summaries_test_df['wording'] = y_test_pred_wording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_test_df[['student_id', 'content', 'wording']].to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
